---
title: "318_Final_Project"
author: "Miranda Miao"
date: "12/18/2020"
output:
  word_document: default
  pdf_document: default
---

```{r setup, echo=FALSE, warning=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction and Handling Missing Variables

```{r part1, include = FALSE, echo=FALSE, warning=FALSE, message = FALSE}
library(readstata13)
jarp <- read.dta13("/Users/mirandamiao/Dropbox/Thesis_Miao/Data/Output/jarp_nisei.dta")
head(jarp)
jarp <- jarp[which(jarp$INTERNED==1 & jarp$AGE<47), c("AGE","SEX","MARRIED", "PROPERTY_COMPEN", "PARENT_INC_DISCREET", "COLLEGE", "VIOLENT", "STRIKE", "FORCE")]
jarp$AGE_SQUARE <- jarp$AGE**2
jarp$PARENT_INC_DISCREET[which(jarp$PARENT_INC_DISCREET==2500)]<- 1
jarp$PARENT_INC_DISCREET[which(jarp$PARENT_INC_DISCREET==3750)]<- 1
jarp$PARENT_INC_DISCREET[which(jarp$PARENT_INC_DISCREET==6250)]<- 2
jarp$PARENT_INC_DISCREET[which(jarp$PARENT_INC_DISCREET==8750)]<- 2
jarp$PARENT_INC_DISCREET[which(jarp$PARENT_INC_DISCREET==12500)]<- 3
jarp$PARENT_INC_DISCREET[which(jarp$PARENT_INC_DISCREET==17500)]<- 3
jarp$PARENT_INC_DISCREET[which(jarp$PARENT_INC_DISCREET==24500)]<- 4
jarp$PARENT_INC_DISCREET[which(jarp$PARENT_INC_DISCREET==30000)]<- 4
#Dealing with the missing variable

jarp.clean <- na.exclude(jarp)
names(jarp.clean)[names(jarp.clean) == "PARENT_INC_DISCREET"] <- "PARENT_INC"
jarp.clean$COLLEGE <-  factor(jarp.clean$COLLEGE)
jarp.clean$PARENT_INC <- factor(jarp.clean$PARENT_INC)
rownames(jarp.clean)<- 1:length(jarp.clean$AGE)
attach(jarp.clean)
setwd("/Users/mirandamiao/Dropbox/STATS 318/Data")
write.csv(data.frame(jarp.clean),"jarp_clean_capstone.csv", row.names = FALSE)
#detach(jarp.clean)
```


## Model Selection Usin BIC, AIC, and AUC
```{r part2, echo=TRUE, message=FALSE, warnings = FALSE}
#the full model 
model.full <- glm(COLLEGE~SEX+AGE+AGE_SQUARE+PROPERTY_COMPEN+MARRIED+PARENT_INC+VIOLENT+STRIKE+FORCE,
                    family=binomial(link="logit"),
                    data = jarp.clean)
summary(model.full)

#Use step-wise function AIC
step(model.full, direction = "both", trace = 1, step = 1000, k = 2)
model.AIC <- glm(COLLEGE ~ SEX + AGE + PROPERTY_COMPEN + MARRIED + PARENT_INC + STRIKE + FORCE, 
                    family=binomial(link="logit"),
                    data = jarp.clean)

#use AUC Random Forest Method
library(AUCRF)
set.seed(12)
model.AUC.full <- AUCRF(COLLEGE~SEX+AGE+AGE_SQUARE+PROPERTY_COMPEN+MARRIED+PARENT_INC+VIOLENT+STRIKE+FORCE,
                        data = data.frame(jarp.clean), 
                   k0 = 1, pdel = 0)
model.AUC.full$Kopt
OptimalSet(model.AUC.full)
plot(model.AUC.full, wich="psel", maxvars=20)
model.AUC <- glm(COLLEGE~AGE+AGE_SQUARE+SEX+PARENT_INC+PROPERTY_COMPEN+STRIKE, family = binomial(link = "logit"), data = jarp.clean)
```

## Cross Validation to Choose the Model with Most Predictive Power
```{r part3, echo=TRUE, message=FALSE, warnings = FALSE}

####K FOLD VALIDATION####
###K Fold Cross Validation for AIC Model
#number of units in the data 
n = dim(jarp.clean)[1]
d = floor(n/5)
set.seed(341342)
#shuffle the data before I create subsets
jarp.clean <- jarp.clean[sample(1:length(jarp.clean$COLLEGE), n, replace = FALSE),]
subset <- rep(NA, length(jarp.clean$COLLEGE))
#making the  5 subsets of the data
subset[1:d] <- 1
subset[(d+1):(2*d)] <- 2
subset[(2*d+1):(3*d)] <- 3
subset[(3*d+1):(4*d)] <- 4
subset[(4*d+1):n] <- 5
jarp.clean$SUBSET <- subset
attach(jarp.clean)
#run the for loop to calculate cross validate based on AUC Average for AIC
library(pROC) 
AUC.sum = 0 
for (i in 1:5) {
  model.AIC <- glm(COLLEGE ~SEX + AGE + PROPERTY_COMPEN + MARRIED + PARENT_INC + STRIKE + FORCE,
                   family = binomial(link = "logit"), 
                  data = jarp.clean[-which(jarp.clean$SUBSET == i), ])
  pi.hat <- predict(model.AIC, newdata = jarp.clean[which(jarp.clean$SUBSET == i),])
  #accumulate AUC.sum to take the average later 
  AUC.sum <- AUC.sum + auc(COLLEGE[which(jarp.clean$SUBSET == i)], pi.hat)
  print(auc(COLLEGE[which(jarp.clean$SUBSET == i)], pi.hat))
}

AUC.average.AIC = AUC.sum/5
AUC.average.AIC #0.713


###K Fold Validatin for AUC Model
AUC.sum = 0 
library("pROC") 
for (i in 1:5) {
  model.AUC <- glm(COLLEGE~AGE+AGE_SQUARE+SEX+PARENT_INC+PROPERTY_COMPEN+STRIKE, 
                    family=binomial(link="logit"),
                    data = jarp.clean[-which(jarp.clean$SUBSET == i),])

  pi.hat <- predict(model.AUC, newdata = jarp.clean[which(jarp.clean$SUBSET == i), ])
  #accumulate AUC.sum to take the average later 
  AUC.sum <- AUC.sum + auc(COLLEGE[which(jarp.clean$SUBSET == i)], pi.hat)
  print(auc(COLLEGE[which(jarp.clean$SUBSET == i)], pi.hat))
}

AUC.average.AUC = AUC.sum/5 
AUC.average.AUC #0.706

#assign the final model 
glm.chosen <- glm(COLLEGE ~SEX + AGE + PROPERTY_COMPEN + MARRIED + PARENT_INC + STRIKE + FORCE,
                    family=binomial(link="logit"),
                    data = jarp.clean)
p.hat.chosen <- predict(glm.chosen,  type = "response")
y.hat.chosen <- ifelse(p.hat.chosen >= 0.5, 1, 0)
#create the AUC Curve for the final model
library("pROC") 
plot(roc(jarp.clean$COLLEGE, p.hat.chosen)) #plot the curve 
auc(jarp.clean$COLLEGE, y.hat.chosen) # AUC =0.7258
par(mfrow=c(2,2))
plot(glm.chosen)

```

##Chi Square Test For Model Fit
```{r part4, echo=TRUE, message=FALSE, warnings = FALSE}
#Likelihood Ratio test for Goodness of Fit
summary(glm.chosen)
1-pchisq(1404.4 , df = 1276)
```

## Plot Assumptions

```{r graph, echo=TRUE, message=FALSE, warnings = FALSE}
x_age <- sort(unique(jarp.clean$AGE))
y_age <- rep(NA, length(x_age))

for ( i in 1:length(x_age)){
  
  y_age[i] <- mean(as.numeric(jarp.clean$COLLEGE[which(jarp.clean$AGE == x[i])])-1)
  
}

par(mfrow=c(1,2))
plot(y_age~x_age, xlab= "Age", ylab = "Average Probability of College Degree", main = "Average Rate of College Attainment by Age", col = "blue")
plot(AGE~COLLEGE, ylab = "Age", main = "Boxplot of Age by College Attainment")

x_sex <- c(0,1)
y_sex <- rep(NA, 2)
y_sex[1] <- mean(as.numeric(jarp.clean$COLLEGE[which(jarp.clean$SEX == 0)])-1)
y_sex[2] <- mean(as.numeric(jarp.clean$COLLEGE[which(jarp.clean$SEX == 1)])-1)
boxplot(as.numeric(COLLEGE[which(jarp.clean$SEX == 1)]), as.numeric(COLLEGE[which(jarp.clean$SEX == 0)]))


x_inc <- c(1,2,3,4)
y_inc <- rep(NA,4)
y_inc[1] <- mean(as.numeric(jarp.clean$COLLEGE[which(jarp.clean$PARENT_INC== 1)])-1)
y_inc[2] <- mean(as.numeric(jarp.clean$COLLEGE[which(jarp.clean$PARENT_INC == 2)])-1)
y_inc[3] <- mean(as.numeric(jarp.clean$COLLEGE[which(jarp.clean$PARENT_INC== 3)])-1)
y_inc[4] <- mean(as.numeric(jarp.clean$COLLEGE[which(jarp.clean$PARENT_INC == 4)])-1)
boxplot(as.numeric(PARENT_INC), as.numeric(COLLEGE)-1)

plot(x_inc,y_inc)

plot(x_inc,y_inc, main = "Average Probability of College Attainment by Parental Income Level", ylab = "Average Probability of College Attainment", xlab = "Parental Income Level", col = "red", ylim = c(0,1))
```

## Regression Diagnostics 
```{r part5, echo=TRUE, message=FALSE, warnings = FALSE}
#the residual plots
par(mfrow=c(2,2))
plot(glm.chosen)
#the outliers

####Testing for influential outliers #####
#the threshold for identify outliers
quantile <- apply(dfbetas(glm.chosen),2, quantile)
threshold <- rbind(quantile[2, ]-1.5*(quantile[4, ] - quantile[2, ]), quantile[4, ]+1.5*(quantile[4, ] - quantile[2, ]))
rownames(threshold) <- c("Q1-1.5*IQR", "Q3+1.5*IQR")
#775 due to MARRIED and PARENT_INC2
#1321 due to PARENT_INC4
#19, 102 due to STRIKE
jarp.clean[c(58, 267, 1097), ]
outliers <- dfbetas(glm.chosen)[c(58, 267, 1097), ]
rownames(outliers)<- c(58, 267, 1097)
outliers
threshold

#the influential points are 
jarp.clean <- jarp.clean[-c(58, 267),]
attach(jarp.clean)
#fit the regression without the influential outlier
glm.final.resid <- glm(COLLEGE ~SEX + AGE + PROPERTY_COMPEN + MARRIED + PARENT_INC + STRIKE + FORCE,  
                    family=binomial(link="logit"),
                    data = jarp.clean)
par(mfrow=c(2,2))
plot(glm.final.resid)
summary(glm.final.resid)
1-pchisq(1395.7, df = 1274)
```

##Improve Model By Comparing Nested Model
```{r part6, echo=TRUE, message=FALSE, warnings = FALSE }
glm.full <- glm(COLLEGE ~SEX + AGE + PROPERTY_COMPEN + MARRIED + PARENT_INC + STRIKE + FORCE, 
                    family=binomial(link="logit"),
                    data = jarp.clean)


PARENT_INC2 <- rep(0, length(jarp.clean$COLLEGE))
PARENT_INC2[which(jarp.clean$PARENT_INC==2)] <- 1

PARENT_INC3 <- rep(0, length(jarp.clean$COLLEGE))
PARENT_INC3[which(jarp.clean$PARENT_INC==3)] <- 1

PARENT_INC4 <- rep(0, length(jarp.clean$COLLEGE))
PARENT_INC4[which(jarp.clean$PARENT_INC==4)] <- 1

jarp.clean$PARENT_INC2 <- PARENT_INC2
jarp.clean$PARENT_INC3 <- PARENT_INC3
jarp.clean$PARENT_INC4 <- PARENT_INC4

glm.short <- glm(COLLEGE ~SEX + AGE + PROPERTY_COMPEN + MARRIED + STRIKE + FORCE  + PARENT_INC3 + PARENT_INC4, 
                    family=binomial(link="logit"),
                    data = jarp.clean)

#Testing if Strike and Force are significant or not, likelihood test for model comparison
summary(glm.full)
summary(glm.short)
1-pchisq((1771.5 -1396.0 ),1) #0.438578

glm.short2 <- glm(COLLEGE ~SEX + AGE + PROPERTY_COMPEN + MARRIED + PARENT_INC3 +PARENT_INC4,  
                    family=binomial(link="logit"),
                    data = jarp.clean)
glm.full2 <- glm(COLLEGE ~SEX + AGE + PROPERTY_COMPEN + MARRIED + PARENT_INC3 +PARENT_INC4 + STRIKE + FORCE,  
                    family=binomial(link="logit"),
                    data = jarp.clean)


#Testing if PROPERTY_COMPEN is significant or not, likelihood test for model comparison
summary(glm.short2)
1-pchisq((1773.6-1770.9),1)
```


## Model Interpretation
```{r part7, echo=TRUE, message=FALSE, warnings = FALSE }

glm.final.final <- glm(COLLEGE ~SEX + AGE + PROPERTY_COMPEN + MARRIED + STRIKE  + PARENT_INC3 + PARENT_INC4, 
                    family=binomial(link="logit"),
                    data = jarp.clean[which(AGE<45),])
summary(glm.final.final)

1-pchisq(1191.1 ,1055)

p.hat.final <- predict(glm.final.final,  type = "response")
y.hat.final<- ifelse(p.hat.final >= 0.5, 1, 0)

library("pROC") 
plot(roc(COLLEGE, p.hat.final)) #plot the curve 
auc(COLLEGE, p.hat.final) # AUC =0.7258
```



